library(corrplot)
library(gplots)
library(kohonen)
library(plot3D)
library(factoextra)
library(stringr)
library(dplyr)
library(dendextend)
library(ape)
library(RColorBrewer)
library(rmarkdown)
library(DataExplorer)
library(tidyverse)
library(kohonen)
# library(checkpoint)  # not working properly @Ramon
# checkpoint("2019-09-20")  # not working properly @Ramon
# clean the memory
rm(list = ls(all = TRUE))
# load data
df <- read.csv("winequality-red.csv")
# count the missing values (NA) in the data frame
sum(is.na(df))
# check if the columns are numeric
sapply(df, is.numeric)
library(corrplot)
library(gplots)
library(kohonen)
library(plot3D)
library(factoextra)
library(stringr)
library(dplyr)
library(dendextend)
library(ape)
library(RColorBrewer)
library(rmarkdown)
library(DataExplorer)
library(tidyverse)
library(kohonen)
# library(checkpoint)  # not working properly @Ramon
# checkpoint("2019-09-20")  # not working properly @Ramon
# clean the memory
rm(list = ls(all = TRUE))
# load data
df <- read.csv("winequality-red.csv")
# count the missing values (NA) in the data frame
sum(is.na(df))
# check if the columns are numeric
sapply(df, is.numeric)
# View(df)
create_report(df)  # not working @Ramon, works @Lars
# Run K-means
km.out <- kmeans(df_scaled, 2,nstart =50)
df_scaled <- scale(df)
df_scaled
# Run K-means
km.out <- kmeans(df_scaled, 2,nstart =50)
plot(df_scaled, col =(km.out$cluster+1), main ="K-Means Clustering", xlab ="", ylab ="", pch =20, cex =2)
# Check the outcome
km.out
km.out$withinss
km.out$tot.withinss
# Find best value for k
wss <- 0 # intialise
wss_dif <- 0
number_of_clusters_tested <- 20
for (i in 1:number_of_clusters_tested){
km.out <- kmeans(df_scaled,i,nstart =50)
wss[i] <- km.out$tot.withinss
if(i > 1){ # only enter condition for two clusters and higher
wss_dif[i-1] <- wss[i-1]-wss[i] # take difference from previous "total within-cluster sum of squares" and current one.
}
}
plot(1:number_of_clusters_tested, wss, type="b", xlab="Number of Clusters",ylab="Total within-cluster sum of squares")
plot(2:number_of_clusters_tested, wss_dif, type="b", xlab="Number of Clusters",ylab="Difference between Total within-cluster sum of squares")
# Calculate distances between data (default method = euclidean):
distances <- dist(df_scaled, method = "euclidean")
# Compute hierarchical clustering based in distances calculated above:
hc <- hclust(distances)
# Computes dendrogram graphical representation:
dend <- as.dendrogram(hc)
# Graphical representation
plot(dend)
# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
str(dend)
# Graphical representation
plot(dend)
# Transpose data:
df_transposed <- t(df_scaled)
# Calculate distances between data (default method = euclidean):
distances_t <- dist(df_transposed, method = "euclidean")
# Compute hierarchical clustering based in distances calculated above:
hc_t <- hclust(distances_t)
# Computes dendrogram graphical representation:
dend_t <- as.dendrogram(hc_t)
# Graphical representation
plot(dend_t)
hc <- hclust(dist(df_scaled, method = "euclidean"),method="complete")
plot(hc)
# Missing correlation-based methods (spearman, kendall): using library(factoextra) for these methods
# Compute the dissimilarity matrix with different distance types: "euclidean", "manhattan", "pearson", "spearman", "kendall"
res.dist <- get_dist(df_scaled, method = "kendall")
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)
# Compute hierarchical clustering with different linkage types:"single", "complete", "average", "centroid", "ward.D", "ward.D2"
res.hc <- hclust(res.dist, method = "ward")
# Visualize the tree
fviz_dend(res.hc)
# Or simply
plot(res.hc)
# Create heatmap
heatmap(df_scaled)
# Make your own color code brewer.pal(n, name)
# n Number of different colors in the palette, minimum 3, maximum depending on palette
# name Blues BuGn BuPu GnBu Greens Greys Oranges OrRd PuBu PuBuGn PuRd Purples RdPu Reds YlGn YlGnBu YlOrBr YlOrRd
palette <- colorRampPalette(brewer.pal(9, "YlGnBu"))
# Row- and column-wise clustering, with wished linkage and distance method:
hc1 <- hclust(as.dist(1-cor(t(df_scaled), method="kendall")), method="ward")
# Row- and column-wise clustering, with wished linkage and distance method:
hc1 <- hclust(as.dist(1-cor(t(df_scaled), method="kendall")), method="ward.D2")
hc2 <- hclust(as.dist(1-cor(df_scaled, method="spearman")), method="single")
# Plot heatmap
heatmap.2(df_scaled, Rowv=as.dendrogram(hc1), Colv=as.dendrogram(hc2), col=palette,scale="row", density.info="none", trace="none")
# Compute PCA.
# scale=TRUE to scale the variables to have standard deviation = 1
pca_out = prcomp(df, scale=TRUE)
# Show available metrics computed by PCA:
names(pca_out)
# Access metrics computed by PCA
pca_out$sdev    # show sdev
pca_out$center  # show mean
pca_out$scale   # show scale
# Rotation matrix provides the principal component of the loadings.
dim(pca_out$rotation) # p*p matrix whereas p is the number of variables (loadings)
pca_out$rotation
# x matrix provides the principal component of the scores.
dim(pca_out$x) # n * p matrix whereas n is the number of observations and p is the number of principal componentes
# pca_out$x # not executed due to size of the matrix
# Create Biplot
# scale=0 ensures that the arrows are scaled to represent the loadings;
# other values for scale give slightly different biplots with different interpretations.
biplot(pca_out,scale=0)
# Create Biplot with reduced sample to get a better overview
# Sample Data
set.seed(2)
sample_1=sample(1:nrow(df), nrow(df)/3) # sample a third of the data for training data
df_reduced=df[sample_1,]
# Calculate PCA on reduced dataset
pca_out_reduced = prcomp(df_reduced, scale=TRUE)
# Create Biplot on reduced dataset
biplot(pca_out_reduced,scale=0)
?kmeans
?hclust
?heatmap
