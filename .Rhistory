km.out$withinss
cat("withinss =", km.out$tot.withinss)
# Find best value for k
wss <- 0 # intialise
wss_dif <- 0
number_of_clusters_tested <- 20
for (i in 1:number_of_clusters_tested){
km.out <- kmeans(df_scaled,i,nstart =50)
wss[i] <- km.out$tot.withinss
if(i > 1){ # only enter condition for two clusters and higher
wss_dif[i-1] <- wss[i-1]-wss[i] # take difference from previous "total within-cluster sum of squares" and current one.
}
}
plot(1:number_of_clusters_tested, wss, type="b", xlab="Number of Clusters",ylab="Total within-cluster sum of squares")
plot(2:number_of_clusters_tested, wss_dif, type="b", xlab="Number of Clusters",ylab="Difference between Total within-cluster sum of squares")
# Calculate distances between data (default method = euclidean):
distances <- dist(df_scaled, method = "euclidean")
# Compute hierarchical clustering based in distances calculated above:
hc <- hclust(distances)
# Computes dendrogram graphical representation:
dend <- as.dendrogram(hc)
# Graphical representation
plot(dend, main = "Dendogram plot")
# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
# str(dend)  # computionally very expensive, takes about 2 minutes
# Transpose data:
df_transposed <- t(df_scaled)
# Calculate distances between data (default method = euclidean):
distances_t <- dist(df_transposed, method = "euclidean")
# Compute hierarchical clustering based in distances calculated above:
hc_t <- hclust(distances_t)
# Computes dendrogram graphical representation:
dend_t <- as.dendrogram(hc_t)
# Graphical representation
plot(dend_t, main = "Transposed Dendogram plot")
# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
str(dend_t)
# Use a specific linkage or distance methode in hc clustering
# Find more information about distances and which one to choose: https://www.datanovia.com/en/lessons/clustering-distance-measures/
# Possible distance methods: "euclidean", "maximum", "manhattan","canberra","binary", "minkowski"
# Find more information about linkage method and which one to choose: https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/#linkage
# Possible linkage methods: "ward.D", "ward.D2", "single", "complete","average", "mcquitty", "median","centroid"
# Compute hierarchical clustering with euclidean distance and complete method:
hc <- hclust(dist(df_scaled, method = "euclidean"),method="complete")
plot(hc, main = "Dendogram Plot with Euclidean distance, method complete")
# Missing correlation-based methods (spearman, kendall): using library(factoextra) for these methods
# Compute the dissimilarity matrix with different distance types: "euclidean", "manhattan", "pearson", "spearman", "kendall"
res.dist <- get_dist(df_scaled, method = "kendall")
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)  # takes some time ~1min
# Compute hierarchical clustering with different linkage types:"single", "complete", "average", "centroid", "ward.D", "ward.D2"
res.hc <- hclust(res.dist, method = "ward")
# Visualize the tree
fviz_dend(res.hc)
# Or simply
plot(res.hc, main = "HC with ward linkage type")
# Create heatmap with scaled data
heatmap(df_scaled)
# Make your own color code brewer.pal(n, name)
# n Number of different colors in the palette, minimum 3, maximum depending on palette
# name Blues BuGn BuPu GnBu Greens Greys Oranges OrRd PuBu PuBuGn PuRd Purples RdPu Reds YlGn YlGnBu YlOrBr YlOrRd
palette <- colorRampPalette(brewer.pal(9, "YlGnBu"))
# Row- and column-wise clustering, with wished linkage and distance method; hc1 with regular data and hc2 with transposed data:
hc1 <- hclust(as.dist(1-cor(t(df_scaled), method="kendall")), method="ward.D2")
hc2 <- hclust(as.dist(1-cor(df_scaled, method="spearman")), method="single")
# Plot heatmap: Heatmap.2 (included in the Gplots Library) allows to define linkage & distance methods for heatmap representaton
heatmap.2(df_scaled, Rowv=as.dendrogram(hc1), Colv=as.dendrogram(hc2), col=palette,scale="row", density.info="none", trace="none")
# the dateframe needed to be re-defined as dataframe for the code to work properly
df_scaled_df <- as.data.frame(df_scaled)
# datapoint later in the plot: data_label<-as.factor(rownames(data))
# Run tSNE:
# we need to remove duplicates
tsne <- Rtsne(df_scaled_df[!duplicated(df_scaled_df), ], dims = 2,
perplexity=50, verbose=TRUE,
max_iter = 500)
## 1 ## This first section concerning colouring allows us to colour scaled numeric columns
#Create a function to generate a continuous color palette
rbPal <- colorRampPalette(c('red','yellow'))
#This adds a column of color values
# creates 10 colour buckets based on quality
df_scaled_df$Col_qual <- rbPal(8)[as.numeric(cut(df_scaled_df$quality,breaks = 10))]
# creates 10 colour buckets based on ph
df_scaled_df$Col_pH <- rbPal(8)[as.numeric(cut((df_scaled_df$pH),breaks = 10))]
## 2 ## We can choose colouring to be df_scaled_df$Col or e.g. df$quality
# Plot data and labels:
plot(tsne$Y, col=df_scaled_df$Col_qual, pch=16, main = "tSNE coloured by quality")
plot(tsne$Y, col=df_scaled_df$Col_pH, pch=16, main = "tSNE coloured by pH")
str(tsne)
# Compute PCA.
# scale=TRUE to scale the variables to have standard deviation = 1
pca_out = prcomp(df, scale=TRUE)
# Show available metrics computed by PCA:
names(pca_out)
# Access metrics computed by PCA
pca_out$sdev    # show sdev
pca_out$center  # show mean
pca_out$scale   # show scale
# Rotation matrix provides the principal component of the loadings.
dim(pca_out$rotation) # p*p matrix whereas p is the number of variables (loadings)
round(pca_out$rotation, digit=2) # show loadings for each predictor. Round result on 2 digits.
# x matrix provides the principal component of the scores.
dim(pca_out$x) # n * p matrix whereas n is the number of observations and p is the number of principal componentes
# pca_out$x # not executed due to size of the matrix
# Create Biplot
# scale=0 ensures that the arrows are scaled to represent the loadings;
# other values for scale give slightly different biplots with different interpretations.
# cex (character expension factor): configures the labeling size of the observations (black) and the predictors (red)
# cex: reduze labeling size of the observations to 0.5 in order to make the plot more readable.
biplot(pca_out,scale=0, cex=c(0.5,1))
plot(df$quality,df$alcohol, main="Quality-Alcohol-Plot")
plot(df$fixed.acidity,df$pH, main="Acidity-pH-Plot")
plot(df$quality,df$total.sulfur.dioxide, main="Quality-Sulfur-Plot")
plot(df$chlorides,df$density, main="Chlorids-Density-Plot")
# screeplot shows the variance of each prinicipal component
screeplot(pca_out)
# Alternative:
# It's more usueful to have the proportion (%) of variance of each principal component
# Numeric values about proportion of variance can be retrieved with the summary function
summary(pca_out)
# Compute the proportion of variance explained by each principal component
# Calculation: variance explained by each principal component / total variance explained by all principal components)
# Calculate variance of prinicipal components
pca_out_var <- pca_out$sdev^2
# calculate proportion of variance for each principal component
pve <- pca_out_var/sum(pca_out_var)
pve
# plots
plot(pve, main="Proporation of Variance per PC",xlab="Principal Component",ylab="Proportion of Variance Explained",ylim=c(0,1),type='b')
plot(cumsum(pve),main="Cumulative Proporation of Variance per PC",xlab="PrincipalComponent",ylab="Cumulative Proportion of Variance
Explained",ylim=c(0,1),type='b')
# preprocess data for model
df_unique <- unique(df)  # remove duplicates, here 200 data rows
glimpse(df_unique)
data <-
as.matrix(scale(df_unique[, 1:11]))  # scale: mean = 0, sd = 1
# show feature names
dimnames(data)[2]  # label successfully removed
# For plotting evaluation against colorcode
# category (~ classification solution)
row_label <- as.factor(rownames(data))
# qualities <- as.character(df$quality)
colors <- c("empty", "empty", "violet", "green", "blue", "red", "orange", "black")
colors <- colors[df$quality]
data_train_matrix <- as.matrix(scale(data))
# set up model
set.seed(22)  # for reproducible results
# get heuristic number of neurons for the following grid
# use this heuristic from Alexander Maier https://www.researchgate.net/post/How_many_nodes_for_self-organizing_maps
neurons <- 5*sqrt(nrow(df))
neuron_per_grid <- round(sqrt(neurons))
# Define the neuronal grid
som_grid <- somgrid(xdim = neuron_per_grid, ydim = neuron_per_grid,
topo = "hexagonal")
# this grid is not really usable because the interpretation is not really possible, so one would choose a really low level for the neurons
# Redefine the neuronal grid
som_grid <- somgrid(xdim = 4, ydim = 4,
topo = "hexagonal")
# Train the model
som_model <- som(
data_train_matrix,
grid = som_grid,
rlen = 1000,  # number of iterations, tried with 10'000 and 100'000 but with no significant better result. Always approches minimum after two third of the iterations
alpha = c(0.05, 0.01),  # learning rate
keep.data = TRUE
)
summary(som_model)
# Check training progress
options(scipen = 999)  # for better reading
plot(som_model, type = "changes")
# Check how many samples are mapped to each
# node on the map. (5-10 samples per node)
# Explore training results
plot(som_model, type = "count")  # how many datapoints are in a neuron
plot(som_model, type = "mapping",  # show datapoints per neuron, 5 - 10 datapoints
col = colors[row_label])      # per neuron is the target range
# one color per wine quality
plot(  # show sample of datapoints, wine quality and number of row
som_model,
type = "mapping",
labels = (rownames(data)),
col = colors[row_label]
)
# U-Matrix: measure of distance between each node and its neighbours.
# (Euclidean distance between weight vectors of neighboring neurons)
# Can be used to identify clusters/boundaries within the SOM map.
# Areas of low neighbour distance ~ groups of nodes that are similar.
plot(som_model, type = "dist.neighbours")
# Codes / Weight vectors: representative of the samples mapped to a node.
# highlights patterns in the distribution of samples and variables.
plot(som_model, type = "codes")
# Heatmaps: identify interesting areas on the map.
# Visualise the distribution of a single variable (defined in [,x]) across the map
plot(som_model,
type = "property",
property = getCodes(som_model, 1)[, 2])
# Same as above but with original, unscaled data (can also be useful)
var_unscaled <- aggregate(
as.numeric(df_unique[, 1]),
by = list(som_model$unit.classif),
FUN = mean,
simplify = TRUE
)[, 2]
plot(som_model, type = "property", property = var_unscaled)
# Clustering: isolate groups of samples with similar metrics
tree <-
as.dendrogram(hclust(dist(as.numeric(
unlist(som_model$codes)
))))
plot(tree, ylab = "Height (h)")
# Cut the tree somewhere based on the above tree
som_cluster <-
cutree(hclust(dist(as.numeric(
unlist(som_model$codes)
))),
h = 2) # k groups or at h hight
# Visualize mapping based on HC
pretty_palette <- c("#1f77b4",
'#ff7f0e',
'#2ca02c',
'#d62728',
'#9467bd',
'#8c564b',
'#e377c2')
plot(
som_model,
type = "mapping",
labels = (rownames(data)),
bgcol = pretty_palette[som_cluster],
col = colors[row_label]
)
add.cluster.boundaries(som_model, som_cluster)
# Create Biplot
# scale=0 ensures that the arrows are scaled to represent the loadings;
# other values for scale give slightly different biplots with different interpretations.
# cex (character expension factor): configures the labeling size of the observations (black) and the predictors (red)
# cex: reduze labeling size of the observations to 0.5 in order to make the plot more readable.
biplot(pca_out,scale=0, cex=c(0.3,1))
library(Rtsne) # library for TSNE
library(corrplot) # it is a graphical display of a correlation matrix, confidence interval
library(gplots)  # for making own color code (e.g. for heatmap representation)
library(kohonen) # functions to train self-organising maps (SOMs)
library(plot3D) # functions for viewing 2-D & 3-D data, including perspective plots, slice plots, scatter plots, etc.
library(factoextra)  # more complexe library than standard R library; all methods for distance & linkage available
library(dendextend) # can be used to represent the data in “fan” diagrams
library(ape) # can be used to represent the data in “fan” diagrams
library(RColorBrewer)  # for making own color code and show different shades of colors (e.g. for heatmap representation)
library(rmarkdown) # convert R Markdown documents into a variety of formats
library(DataExplorer) # automated data exploration process for analytic tasks and predictive modeling
library(tidyverse)  # library for SOM
library(kohonen)  # library for SOM
library(checkpoint) # package to solve the problem of package reproducibility in R
knitr::include_graphics("documents/Methods picture.PNG")
# clean the memory
rm(list = ls(all = TRUE))
# load data
df <- read.csv("winequality-red.csv")
saveRDS(df, "data/winequality-red.Rds")
# count the missing values (NA) in the data frame
sum(is.na(df))
# check if the columns are numeric
sapply(df, is.numeric)
# scale the data
df_scaled <- scale(df)
class(df_scaled)  # is now a matrix
as_tibble(df_scaled)
glimpse(df)
# View(df)
summary(df)
# create_report(df)
# Run K-means for k=2
k <- 2 #defining to work with two clusters
km.out <- kmeans(df_scaled, k,nstart =50)
plot(df_scaled, col =(km.out$cluster+1), main ="K-Means Clustering", xlab ="", ylab ="", pch =20, cex =2)
# Check the outcome
km.out
km.out$withinss
cat("withinss =", km.out$tot.withinss)
# Find best value for k
wss <- 0 # intialise
wss_dif <- 0
number_of_clusters_tested <- 20
for (i in 1:number_of_clusters_tested){
km.out <- kmeans(df_scaled,i,nstart =50)
wss[i] <- km.out$tot.withinss
if(i > 1){ # only enter condition for two clusters and higher
wss_dif[i-1] <- wss[i-1]-wss[i] # take difference from previous "total within-cluster sum of squares" and current one.
}
}
plot(1:number_of_clusters_tested, wss, type="b", xlab="Number of Clusters",ylab="Total within-cluster sum of squares")
plot(2:number_of_clusters_tested, wss_dif, type="b", xlab="Number of Clusters",ylab="Difference between Total within-cluster sum of squares")
# Calculate distances between data (default method = euclidean):
distances <- dist(df_scaled, method = "euclidean")
# Compute hierarchical clustering based in distances calculated above:
hc <- hclust(distances)
# Computes dendrogram graphical representation:
dend <- as.dendrogram(hc)
# Graphical representation
plot(dend, main = "Dendogram plot")
# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
# str(dend)  # computionally very expensive, takes about 2 minutes
# Transpose data:
df_transposed <- t(df_scaled)
# Calculate distances between data (default method = euclidean):
distances_t <- dist(df_transposed, method = "euclidean")
# Compute hierarchical clustering based in distances calculated above:
hc_t <- hclust(distances_t)
# Computes dendrogram graphical representation:
dend_t <- as.dendrogram(hc_t)
# Graphical representation
plot(dend_t, main = "Transposed Dendogram plot")
# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
str(dend_t)
# Use a specific linkage or distance methode in hc clustering
# Find more information about distances and which one to choose: https://www.datanovia.com/en/lessons/clustering-distance-measures/
# Possible distance methods: "euclidean", "maximum", "manhattan","canberra","binary", "minkowski"
# Find more information about linkage method and which one to choose: https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/#linkage
# Possible linkage methods: "ward.D", "ward.D2", "single", "complete","average", "mcquitty", "median","centroid"
# Compute hierarchical clustering with euclidean distance and complete method:
hc <- hclust(dist(df_scaled, method = "euclidean"),method="complete")
plot(hc, main = "Dendogram Plot with Euclidean distance, method complete")
# Missing correlation-based methods (spearman, kendall): using library(factoextra) for these methods
# Compute the dissimilarity matrix with different distance types: "euclidean", "manhattan", "pearson", "spearman", "kendall"
res.dist <- get_dist(df_scaled, method = "kendall")
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)  # takes some time ~1min
# Compute hierarchical clustering with different linkage types:"single", "complete", "average", "centroid", "ward.D", "ward.D2"
res.hc <- hclust(res.dist, method = "ward")
# Visualize the tree
fviz_dend(res.hc)
# Or simply
plot(res.hc, main = "HC with ward linkage type")
# Create heatmap with scaled data
heatmap(df_scaled)
# Make your own color code brewer.pal(n, name)
# n Number of different colors in the palette, minimum 3, maximum depending on palette
# name Blues BuGn BuPu GnBu Greens Greys Oranges OrRd PuBu PuBuGn PuRd Purples RdPu Reds YlGn YlGnBu YlOrBr YlOrRd
palette <- colorRampPalette(brewer.pal(9, "YlGnBu"))
# Row- and column-wise clustering, with wished linkage and distance method; hc1 with regular data and hc2 with transposed data:
hc1 <- hclust(as.dist(1-cor(t(df_scaled), method="kendall")), method="ward.D2")
hc2 <- hclust(as.dist(1-cor(df_scaled, method="spearman")), method="single")
# Plot heatmap: Heatmap.2 (included in the Gplots Library) allows to define linkage & distance methods for heatmap representaton
heatmap.2(df_scaled, Rowv=as.dendrogram(hc1), Colv=as.dendrogram(hc2), col=palette,scale="row", density.info="none", trace="none")
# the dateframe needed to be re-defined as dataframe for the code to work properly
df_scaled_df <- as.data.frame(df_scaled)
# datapoint later in the plot: data_label<-as.factor(rownames(data))
# Run tSNE:
# we need to remove duplicates
tsne <- Rtsne(df_scaled_df[!duplicated(df_scaled_df), ], dims = 2,
perplexity=50, verbose=TRUE,
max_iter = 500)
## 1 ## This first section concerning colouring allows us to colour scaled numeric columns
#Create a function to generate a continuous color palette
rbPal <- colorRampPalette(c('red','yellow'))
#This adds a column of color values
# creates 10 colour buckets based on quality
df_scaled_df$Col_qual <- rbPal(8)[as.numeric(cut(df_scaled_df$quality,breaks = 10))]
# creates 10 colour buckets based on ph
df_scaled_df$Col_pH <- rbPal(8)[as.numeric(cut((df_scaled_df$pH),breaks = 10))]
## 2 ## We can choose colouring to be df_scaled_df$Col or e.g. df$quality
# Plot data and labels:
plot(tsne$Y, col=df_scaled_df$Col_qual, pch=16, main = "tSNE coloured by quality")
plot(tsne$Y, col=df_scaled_df$Col_pH, pch=16, main = "tSNE coloured by pH")
str(tsne)
# Compute PCA.
# scale=TRUE to scale the variables to have standard deviation = 1
pca_out = prcomp(df, scale=TRUE)
# Show available metrics computed by PCA:
names(pca_out)
# Access metrics computed by PCA
pca_out$sdev    # show sdev
pca_out$center  # show mean
pca_out$scale   # show scale
# Rotation matrix provides the principal component of the loadings.
dim(pca_out$rotation) # p*p matrix whereas p is the number of variables (loadings)
round(pca_out$rotation, digit=2) # show loadings for each predictor. Round result on 2 digits.
# x matrix provides the principal component of the scores.
dim(pca_out$x) # n * p matrix whereas n is the number of observations and p is the number of principal componentes
# pca_out$x # not executed due to size of the matrix
# Create Biplot
# scale=0 ensures that the arrows are scaled to represent the loadings;
# other values for scale give slightly different biplots with different interpretations.
# cex (character expension factor): configures the labeling size of the observations (black) and the predictors (red)
# cex: reduze labeling size of the observations to 0.5 in order to make the plot more readable.
biplot(pca_out,scale=0, cex=c(0.3,1))
plot(df$quality,df$alcohol, main="Quality-Alcohol-Plot")
plot(df$fixed.acidity,df$pH, main="Acidity-pH-Plot")
plot(df$quality,df$total.sulfur.dioxide, main="Quality-Sulfur-Plot")
plot(df$chlorides,df$density, main="Chlorids-Density-Plot")
# screeplot shows the variance of each prinicipal component
screeplot(pca_out)
# Alternative:
# It's more usueful to have the proportion (%) of variance of each principal component
# Numeric values about proportion of variance can be retrieved with the summary function
summary(pca_out)
# Compute the proportion of variance explained by each principal component
# Calculation: variance explained by each principal component / total variance explained by all principal components)
# Calculate variance of prinicipal components
pca_out_var <- pca_out$sdev^2
# calculate proportion of variance for each principal component
pve <- pca_out_var/sum(pca_out_var)
pve
# plots
plot(pve, main="Proporation of Variance per PC",xlab="Principal Component",ylab="Proportion of Variance Explained",ylim=c(0,1),type='b')
plot(cumsum(pve),main="Cumulative Proporation of Variance per PC",xlab="PrincipalComponent",ylab="Cumulative Proportion of Variance
Explained",ylim=c(0,1),type='b')
# preprocess data for model
df_unique <- unique(df)  # remove duplicates, here 200 data rows
glimpse(df_unique)
data <-
as.matrix(scale(df_unique[, 1:11]))  # scale: mean = 0, sd = 1
# show feature names
dimnames(data)[2]  # label successfully removed
# For plotting evaluation against colorcode
# category (~ classification solution)
row_label <- as.factor(rownames(data))
# qualities <- as.character(df$quality)
colors <- c("empty", "empty", "violet", "green", "blue", "red", "orange", "black")
colors <- colors[df$quality]
data_train_matrix <- as.matrix(scale(data))
# set up model
set.seed(22)  # for reproducible results
# get heuristic number of neurons for the following grid
# use this heuristic from Alexander Maier https://www.researchgate.net/post/How_many_nodes_for_self-organizing_maps
neurons <- 5*sqrt(nrow(df))
neuron_per_grid <- round(sqrt(neurons))
# Define the neuronal grid
som_grid <- somgrid(xdim = neuron_per_grid, ydim = neuron_per_grid,
topo = "hexagonal")
# this grid is not really usable because the interpretation is not really possible, so one would choose a really low level for the neurons
# Redefine the neuronal grid
som_grid <- somgrid(xdim = 4, ydim = 4,
topo = "hexagonal")
# Train the model
som_model <- som(
data_train_matrix,
grid = som_grid,
rlen = 1000,  # number of iterations, tried with 10'000 and 100'000 but with no significant better result. Always approches minimum after two third of the iterations
alpha = c(0.05, 0.01),  # learning rate
keep.data = TRUE
)
summary(som_model)
# Check training progress
options(scipen = 999)  # for better reading
plot(som_model, type = "changes")
# Check how many samples are mapped to each
# node on the map. (5-10 samples per node)
# Explore training results
plot(som_model, type = "count")  # how many datapoints are in a neuron
plot(som_model, type = "mapping",  # show datapoints per neuron, 5 - 10 datapoints
col = colors[row_label])      # per neuron is the target range
# one color per wine quality
plot(  # show sample of datapoints, wine quality and number of row
som_model,
type = "mapping",
labels = (rownames(data)),
col = colors[row_label]
)
# U-Matrix: measure of distance between each node and its neighbours.
# (Euclidean distance between weight vectors of neighboring neurons)
# Can be used to identify clusters/boundaries within the SOM map.
# Areas of low neighbour distance ~ groups of nodes that are similar.
plot(som_model, type = "dist.neighbours")
# Codes / Weight vectors: representative of the samples mapped to a node.
# highlights patterns in the distribution of samples and variables.
plot(som_model, type = "codes")
# Heatmaps: identify interesting areas on the map.
# Visualise the distribution of a single variable (defined in [,x]) across the map
plot(som_model,
type = "property",
property = getCodes(som_model, 1)[, 2])
# Same as above but with original, unscaled data (can also be useful)
var_unscaled <- aggregate(
as.numeric(df_unique[, 1]),
by = list(som_model$unit.classif),
FUN = mean,
simplify = TRUE
)[, 2]
plot(som_model, type = "property", property = var_unscaled)
# Clustering: isolate groups of samples with similar metrics
tree <-
as.dendrogram(hclust(dist(as.numeric(
unlist(som_model$codes)
))))
plot(tree, ylab = "Height (h)")
# Cut the tree somewhere based on the above tree
som_cluster <-
cutree(hclust(dist(as.numeric(
unlist(som_model$codes)
))),
h = 2) # k groups or at h hight
# Visualize mapping based on HC
pretty_palette <- c("#1f77b4",
'#ff7f0e',
'#2ca02c',
'#d62728',
'#9467bd',
'#8c564b',
'#e377c2')
plot(
som_model,
type = "mapping",
labels = (rownames(data)),
bgcol = pretty_palette[som_cluster],
col = colors[row_label]
)
add.cluster.boundaries(som_model, som_cluster)
# Create Biplot
# scale=0 ensures that the arrows are scaled to represent the loadings;
# other values for scale give slightly different biplots with different interpretations.
# cex (character expension factor): configures the labeling size of the observations (black) and the predictors (red)
# cex: reduze labeling size of the observations to 0.5 in order to make the plot more readable.
biplot(pca_out,scale=0, cex=c(0.3,0.8))
