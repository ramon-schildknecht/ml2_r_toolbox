---
title: "group_work"
output:
  pdf_document: default
  html_document: default
---
Lecture 1: Kmeans clustering and Hierarchical Clustering (HC) & Heat Maps

# Setup
## Install packages

```{r install_packages, message=FALSE, warning=FALSE}
library(corrplot)
library(gplots)
library(kohonen)
library(plot3D)
library(factoextra)
library(stringr)
library(dplyr)
library(dendextend)
library(ape)
library(RColorBrewer)
library(rmarkdown)
library(DataExplorer)
library(tidyverse)
# library(checkpoint)  # not working properly @Ramon
```


## Set checkpoint

```{r set checkpoint, message=FALSE, warning=FALSE}
# checkpoint("2019-09-20")  # not working properly @Ramon
```



# Data 
## Preparation
```{r preparation}
# clean the memory
rm(list = ls(all = TRUE)) 

# load data
df <- read.csv("winequality-red.csv")

# count the missing values (NA) in the data frame
sum(is.na(df))

# check if the columns are numeric
sapply(df, is.numeric)

```


## Exploration

```{r exploration}
glimpse(df)
# View(df)
create_report(df)  # not working @Ramon, works @Lars
```



```{r}
# scale the data
df_scaled <- scale(df)
df_scaled

```
```{r}
### Methode 1: K-Means-Clustering

# Scale data
df_scaled <- scale(df)
df_scaled

# Run K-means
km.out <- kmeans(df_scaled, 2,nstart =50)
plot(df_scaled, col =(km.out$cluster+1), main ="K-Means Clustering", xlab ="", ylab ="", pch =20, cex =2)

# Check the outcome
km.out
km.out$withinss
km.out$tot.withinss

# Find best value for k
wss <- 0 # intialise
wss_dif <- 0
number_of_clusters_tested <- 20
for (i in 1:number_of_clusters_tested){
  km.out <- kmeans(df_scaled,i,nstart =50)
  wss[i] <- km.out$tot.withinss
  if(i > 1){ # only enter condition for two clusters and higher
    wss_dif[i-1] <- wss[i-1]-wss[i] # take difference from previous "total within-cluster sum of squares" and current one.
  }
    
}

plot(1:number_of_clusters_tested, wss, type="b", xlab="Number of Clusters",ylab="Total within-cluster sum of squares")
plot(2:number_of_clusters_tested, wss_dif, type="b", xlab="Number of Clusters",ylab="Difference between Total within-cluster sum of squares")
```
First plot: K-means Clustering with k=2 is not very useful. This plot is only useful if you have a dataset with two dimensions.

Second plot: With increasing number of clusters the total within cluster sum of squares decreases always. To decide which k is the best look for the ellbow in the plot. In this plot there is no clear ellbow visible. Therefore we go for k = 2.

Third plot: This plot shows the differnce between the i-th and the i-1th value of the total within-cluster sum of squares. This plot should be enjoyed with caution.



```{r}
### Methode 2: Hierarchical Clustering

# Scale data
df_scaled <- scale(df)
df_scaled

# Calculate distances between data (default method = euclidean):
distances <- dist(df_scaled, method = "euclidean")

# Compute hierarchical clustering based in distances calculated above:
hc <- hclust(distances)

# Computes dendrogram graphical representation:
dend <- as.dendrogram(hc)

# Graphical representation
plot(dend)

# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
str(dend)



## Transpose data:
df_transposed <- t(df_scaled)

# Calculate distances between data (default method = euclidean):
distances_t <- dist(df_transposed, method = "euclidean")

# Compute hierarchical clustering based in distances calculated above:
hc_t <- hclust(distances_t)

# Computes dendrogram graphical representation:
dend_t <- as.dendrogram(hc_t)

# Graphical representation
plot(dend_t)

# Alternative, standard output representation (can be useful for ctrl+find specific things in big trees)
str(dend_t)




## Use a specific linkage or distance methode in hc clustering

# Possible linkage methods: "ward.D", "ward.D2", "single", "complete","average", "mcquitty", "median","centroid"
# Possible distance methods: "euclidean", "maximum", "manhattan","canberra","binary", "minkowski"

hc <- hclust(dist(df_scaled, method = "euclidean"),method="complete")
plot(hc)


# Missing correlation-based methods (spearman, kendall): using library(factoextra) for these methods
# Compute the dissimilarity matrix with different distance types: "euclidean", "manhattan", "pearson", "spearman", "kendall"
res.dist <- get_dist(df_scaled, method = "kendall")

# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)

# Compute hierarchical clustering with different linkage types:"single", "complete", "average", "centroid", "ward.D", "ward.D2"
res.hc <- hclust(res.dist, method = "ward")

# Visualize the tree
fviz_dend(res.hc)

# Or simply
plot(res.hc)



### Methode 3: Heatmap

# Scale data
df_scaled <- scale(df)
df_scaled

# Create heatmap
heatmap(df_scaled)

# Make your own color code brewer.pal(n, name)
# n Number of different colors in the palette, minimum 3, maximum depending on palette
# name Blues BuGn BuPu GnBu Greens Greys Oranges OrRd PuBu PuBuGn PuRd Purples RdPu Reds YlGn YlGnBu YlOrBr YlOrRd
palette <- colorRampPalette(brewer.pal(9, "YlGnBu"))

# Row- and column-wise clustering, with wished linkage and distance method:
hc1 <- hclust(as.dist(1-cor(t(df_scaled), method="kendall")), method="ward")
hc2 <- hclust(as.dist(1-cor(df_scaled, method="spearman")), method="single")

# Plot heatmap
heatmap.2(df_scaled, Rowv=as.dendrogram(hc1), Colv=as.dendrogram(hc2), col=palette,scale="row", density.info="none", trace="none")

```
Second plot: 

#TSNE
```{r}
library(Rtsne)
# datapoint later in the plot: data_label<-as.factor(rownames(data))
# Run tSNE:

# we need to remove duplicates

tsne <- Rtsne(df_scaled[!duplicated(df_scaled), ], dims = 2,
            perplexity=30, verbose=TRUE,
            max_iter = 500)

# Use table row names to label the
# datapoint later in the plot: 
data_label<-as.factor(colnames(df_scaled))

# Plot data and labels:
plot(tsne$Y, col=data_label)

str(tsne)
```


# Prinicpal Component Analysis (PCA)

## Caluclate PCA and Show Metrics

**Available metrics in the returend PCA object**  

* **sdev:**  standard deviation of the principal components
* **rotation:** rotation matrix shows the principal components of the loadings (variables). 
* **center:** mean of each variable before scaling and computing PCA
* **scale:** applied scale
* **x:** x matrix shows the principal components of the scores (of the observation)

```{r}

# Compute PCA.
# scale=TRUE to scale the variables to have standard deviation = 1
pca_out = prcomp(df, scale=TRUE)

# Show available metrics computed by PCA: 
names(pca_out)

# Access metrics computed by PCA
pca_out$sdev    # show sdev
pca_out$center  # show mean
pca_out$scale   # show scale

# Rotation matrix provides the principal component of the loadings.
dim(pca_out$rotation) # p*p matrix whereas p is the number of variables (loadings)
pca_out$rotation

# x matrix provides the principal component of the scores.
dim(pca_out$x) # n * p matrix whereas n is the number of observations and p is the number of principal componentes
# pca_out$x # not executed due to size of the matrix
```

## Create Biplot

**Interpretation: **
  
* The features quality and aclcohol are similar
* free.sulfur dioxide and total.sulfur dioxide point into the opposite direction as quality. So it can be expected that sulfur content of a wine is an indicator for bad quality. -> not true according to plot further below
* fixed.acitiy and pH point in the opposite direction. pH value is a metric to measure the acidity. A low pH value indicates high acidity whereas a high pH value indicate a low acidity (alkaline). So the result in the biplot is conclusive
```{r}

# Create Biplot 
# scale=0 ensures that the arrows are scaled to represent the loadings; 
# other values for scale give slightly different biplots with different interpretations.
biplot(pca_out,scale=0)


# Create Biplot with reduced sample to get a better overview

# Sample Data
set.seed(2)
sample_1=sample(1:nrow(df), nrow(df)/3) # sample a third of the data for training data
df_reduced=df[sample_1,]

# Calculate PCA on reduced dataset
pca_out_reduced = prcomp(df_reduced, scale=TRUE)

# Create Biplot on reduced dataset
biplot(pca_out_reduced,scale=0)

```

## Interpretation of Biplot
```{r}
plot(df$quality,df$alcohol)
plot(df$fixed.acidity,df$pH)
plot(df$quality,df$total.sulfur.dioxide)
plot(df$chlorides,df$density)

```


## Show variance explained by certain Principal Components

**summary() on a prcomp object**
Summary shows the standard deviation, proportion of variance and cumulative proportion of variance of each principal component
  
**Result:** 6 Principal Components explain more than 80% of the variance in the data.
```{r}
# screeplot shows the variance of each prinicipal component
screeplot(pca_out)

# Alternative:
# It's more usueful to have the proportion (%) of variance of each principal component

# Numeric values about proportion of variance can be retrieved with the summary function
summary(pca_out)

# Compute the proportion of variance explained by each principal component 
# Calculation: variance explained by each principal component / total variance explained by all principal components)

# Calculate variance of prinicipal components
pca_out_var <- pca_out$sdev^2

# calculate proportion of variance for each principal component
pve <- pca_out_var/sum(pca_out_var)
pve

# plots
plot(pve, main="Proporation of Variance per PC",xlab="Principal Component",ylab="Proportion of Variance Explained",ylim=c(0,1),type='b')
plot(cumsum(pve),main="Cumulative Proporation of Variance per PC",xlab="PrincipalComponent",ylab="Cumulative Proportion of Variance
Explained",ylim=c(0,1),type='b')

```
